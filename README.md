# âš½ Soccer Data Engineering Pipeline.

A hands-on data engineering project that ingests real-time soccer data from a public API, stores raw JSON files, transforms the data into a normalized schema, and loads it into a data warehouse. The project is orchestrated using Apache Airflow and containerized with Docker.

---

## ğŸ“Œ Project Scope

- Ingest soccer match, team, and player data via a public REST API.
- Store raw JSON data in a data lake layer (local or AWS S3).
- Load and normalize data into a PostgreSQL-based warehouse.
- Use dbt to define transformation logic and maintain schema integrity.
- Schedule and orchestrate pipelines using Apache Airflow.
- Containerize all components with Docker for reproducibility.

---

## ğŸ§° Tools & Technologies

| Layer              | Tool                              |
|--------------------|-----------------------------------|
| API Ingestion      | Python + `requests`               |
| Raw Storage        | Local Files / AWS S3              |
| Data Warehouse     | PostgreSQL / DuckDB               |
| Data Modeling      | dbt (Data Build Tool)             |
| Orchestration      | Apache Airflow                    |
| Containerization   | Docker                            |
| Querying & Viz     | DBeaver, pgAdmin, Jupyter, Superset |
| Version Control    | Git + GitHub                      |

---

## ğŸ—‚ Folder Structure

<details>
<summary>ğŸ“ Click to Expand Folder Structure</summary>

```
soccer-pipeline/
â”‚
â”œâ”€â”€ dags/                  # Airflow DAGs
â”œâ”€â”€ dbt/                   # dbt models, snapshots, seeds
â”œâ”€â”€ docker/                # Dockerfiles & docker-compose files
â”œâ”€â”€ notebooks/             # Jupyter notebooks for exploration
â”œâ”€â”€ scripts/               # Python scripts for data ingestion
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Raw JSON files
â”‚   â””â”€â”€ processed/         # Cleaned/structured data
â”œâ”€â”€ .env                   # Environment variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

</details>

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/soccer-pipeline.git
cd soccer-pipeline
```

### 2. Add Environment Variables

Create a `.env` file at the root of the project:

```
API_KEY=your_api_key_here
```

### 3. Run Locally with Docker

```bash
docker-compose up --build
```

- Airflow UI: [http://localhost:8080](http://localhost:8080)
- pgAdmin (if used): [http://localhost:5050](http://localhost:5050)

---

## âœ… Learning Outcomes

- Understand the ELT pipeline workflow
- Practice working with raw JSON APIs and data normalization
- Use dbt to manage models, transformations, and testing
- Schedule automated data workflows using Apache Airflow
- Learn Docker-based development for portability and reproducibility

---

## ğŸ“ˆ Potential Extensions

- Add unit tests with `pytest`
- Schedule CI/CD with GitHub Actions
- Deploy pipeline to AWS (Lambda + S3 + RDS + MWAA)
- Create dashboards using Metabase or Superset

---

## ğŸ§  Author

**Your Name**  
[LinkedIn](https://www.linkedin.com/in/yourprofile)  
[GitHub](https://github.com/your-username)

---

## ğŸ“„ License

This project is licensed under the MIT License.
