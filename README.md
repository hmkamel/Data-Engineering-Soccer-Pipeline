# ⚽ Soccer Data Engineering Pipeline

A hands-on data engineering project that ingests real-time soccer data from a public API, stores raw JSON files, transforms the data into a normalized schema, and loads it into a data warehouse. The project is orchestrated using Apache Airflow and containerized with Docker.

---

## 📌 Project Scope

- Ingest soccer match, team, and player data via a public REST API.
- Store raw JSON data in a data lake layer (local or AWS S3).
- Load and normalize data into a PostgreSQL-based warehouse.
- Use dbt to define transformation logic and maintain schema integrity.
- Schedule and orchestrate pipelines using Apache Airflow.
- Containerize all components with Docker for reproducibility.

---

## 🧰 Tools & Technologies

| Layer              | Tool                              |
|--------------------|-----------------------------------|
| API Ingestion      | Python + `requests`               |
| Raw Storage        | Local Files / AWS S3              |
| Data Warehouse     | PostgreSQL / DuckDB               |
| Data Modeling      | dbt (Data Build Tool)             |
| Orchestration      | Apache Airflow                    |
| Containerization   | Docker                            |
| Querying & Viz     | DBeaver, pgAdmin, Jupyter, Superset |
| Version Control    | Git + GitHub                      |

---

## 🗂 Folder Structure

<details>
<summary>📁 Click to Expand Folder Structure</summary>

```
soccer-pipeline/
│
├── dags/                  # Airflow DAGs
├── dbt/                   # dbt models, snapshots, seeds
├── docker/                # Dockerfiles & docker-compose files
├── notebooks/             # Jupyter notebooks for exploration
├── scripts/               # Python scripts for data ingestion
├── data/
│   ├── raw/               # Raw JSON files
│   └── processed/         # Cleaned/structured data
├── .env                   # Environment variables
├── .gitignore
├── README.md
└── requirements.txt
```

</details>

---

## 🚀 Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/soccer-pipeline.git
cd soccer-pipeline
```

### 2. Add Environment Variables

Create a `.env` file at the root of the project:

```
API_KEY=your_api_key_here
```

### 3. Run Locally with Docker

```bash
docker-compose up --build
```

- Airflow UI: [http://localhost:8080](http://localhost:8080)
- pgAdmin (if used): [http://localhost:5050](http://localhost:5050)

---

## ✅ Learning Outcomes

- Understand the ELT pipeline workflow
- Practice working with raw JSON APIs and data normalization
- Use dbt to manage models, transformations, and testing
- Schedule automated data workflows using Apache Airflow
- Learn Docker-based development for portability and reproducibility

---

## 📈 Potential Extensions

- Add unit tests with `pytest`
- Schedule CI/CD with GitHub Actions
- Deploy pipeline to AWS (Lambda + S3 + RDS + MWAA)
- Create dashboards using Metabase or Superset

---

## 🧠 Author

**Your Name**  
[LinkedIn](https://www.linkedin.com/in/yourprofile)  
[GitHub](https://github.com/your-username)

---

## 📄 License

This project is licensed under the MIT License.
